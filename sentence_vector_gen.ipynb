{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import RepeatVector\n",
    "\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from scipy.stats import describe\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import os\n",
    "from time import gmtime, strftime\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import re\n",
    "\n",
    "# Needed to run only once\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_word2id(word):\n",
    "    try:\n",
    "        return word2id[word]\n",
    "    except KeyError:\n",
    "        return word2id[\"UNK\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_vectors(glove_file, word2id, embed_size):\n",
    "    embedding = np.zeros((len(word2id), embed_size))\n",
    "    fglove = open(glove_file, \"rb\")\n",
    "    for line in fglove:\n",
    "        cols = line.strip().split()\n",
    "        word = cols[0].decode('utf-8')\n",
    "        if embed_size == 0:\n",
    "            embed_size = len(cols) - 1\n",
    "        if word in word2id:\n",
    "            vec = np.array([float(v) for v in cols[1:]])\n",
    "        embedding[lookup_word2id(word)] = vec\n",
    "    embedding[word2id[\"PAD\"]] = np.zeros((embed_size))\n",
    "    embedding[word2id[\"UNK\"]] = np.random.uniform(-1, 1, embed_size)\n",
    "    return embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_generator(X, embeddings, batch_size):\n",
    "    while True:\n",
    "        # loop once per epoch\n",
    "        num_recs = X.shape[0]\n",
    "        indices = np.random.permutation(np.arange(num_recs))\n",
    "        num_batches = num_recs // batch_size\n",
    "        for bid in range(num_batches):\n",
    "            sids = indices[bid * batch_size: (bid + 1) * batch_size]\n",
    "            Xbatch = embeddings[X[sids, :]]\n",
    "            yield Xbatch, Xbatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cosine_similarity(x, y):\n",
    "    return np.dot(x, y) / (np.linalg.norm(x, 2) * np.linalg.norm(y, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"data\"\n",
    "\n",
    "def is_number(n):\n",
    "    temp = re.sub(\"[.,-/]\", \"\",n)\n",
    "    return temp.isdigit()\n",
    "\n",
    "# parsing sentences and building vocabulary\n",
    "word_freqs = collections.Counter()\n",
    "ftext = open(os.path.join(DATA_DIR, \"text.tsv\"), \"r\")\n",
    "sents = []\n",
    "sent_lens = []\n",
    "for line in ftext:\n",
    "    docid, text = line.strip().split(\"\\t\")\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for word in nltk.word_tokenize(sent):\n",
    "            if is_number(word):\n",
    "                word = \"9\"\n",
    "            word = word.lower()\n",
    "            word_freqs[word] += 1\n",
    "        sents.append(sent)\n",
    "        sent_lens.append(len(sent))\n",
    "ftext.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of sentences are: 131574 \n",
      "Sentence distribution min 1, max 2434 , mean 120.624021, median 115.000000\n",
      "Vocab size (full) 50582\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of sentences are: {:d} \".format(len(sents)))\n",
    "print (\"Sentence distribution min {:d}, max {:d} , mean {:3f}, median {:3f}\".\n",
    "      format(np.min(sent_lens), np.max(sent_lens), np.mean(sent_lens), np.median(sent_lens)))\n",
    "print(\"Vocab size (full) {:d}\".format(len(word_freqs)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 5000\n",
    "EMBED_SIZE = 50\n",
    "LATENT_SIZE = 512\n",
    "SEQUENCE_LEN = 50\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary sizes: 5000 5000\n",
      "(5000, 50)\n",
      "number of sentences:  131574\n",
      "(92101, 50) (39473, 50)\n"
     ]
    }
   ],
   "source": [
    "# word2id = collections.defaultdict(lambda: 1)\n",
    "word2id = {}\n",
    "word2id[\"PAD\"] = 0\n",
    "word2id[\"UNK\"] = 1\n",
    "for v, (k, _) in enumerate(word_freqs.most_common(VOCAB_SIZE - 2)):\n",
    "    word2id[k] = v + 2\n",
    "id2word = {v: k for k, v in word2id.items()}\n",
    "\n",
    "print(\"vocabulary sizes:\", len(word2id), len(id2word))\n",
    "\n",
    "sent_wids = [[lookup_word2id(w) for w in s.split()] for s in sents]\n",
    "sent_wids = sequence.pad_sequences(sent_wids, SEQUENCE_LEN)\n",
    "\n",
    "# load glove vectors into weight matrix\n",
    "embeddings = load_glove_vectors(os.path.join(\n",
    "    DATA_DIR, \"glove.6B.{:d}d.txt\".format(EMBED_SIZE)), word2id, EMBED_SIZE)\n",
    "print(embeddings.shape)\n",
    "\n",
    "# split sentences into training and test\n",
    "train_size = 0.7\n",
    "Xtrain, Xtest = train_test_split(sent_wids, train_size=train_size)\n",
    "print(\"number of sentences: \", len(sent_wids))\n",
    "print(Xtrain.shape, Xtest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training and test generators\n",
    "train_gen = sentence_generator(Xtrain, embeddings, BATCH_SIZE)\n",
    "test_gen = sentence_generator(Xtest, embeddings, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define autoencoder network\n",
    "inputs = Input(shape=(SEQUENCE_LEN, EMBED_SIZE), name=\"input\")\n",
    "encoded = Bidirectional(LSTM(LATENT_SIZE), merge_mode=\"sum\",\n",
    "                        name=\"encoder_lstm\")(inputs)\n",
    "decoded = RepeatVector(SEQUENCE_LEN, name=\"repeater\")(encoded)\n",
    "decoded = Bidirectional(LSTM(EMBED_SIZE, return_sequences=True),\n",
    "                        merge_mode=\"sum\",\n",
    "                        name=\"decoder_lstm\")(decoded)\n",
    "\n",
    "autoencoder = Model(inputs, decoded)\n",
    "\n",
    "#tensorboard = make_tensorboard(set_dir_name='rnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\P\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1439/1439 [==============================] - 521s 357ms/step - loss: 0.1404 - val_loss: 0.1243\n",
      "Epoch 2/20\n",
      "1439/1439 [==============================] - 525s 365ms/step - loss: 0.1215 - val_loss: 0.1160\n",
      "Epoch 3/20\n",
      "1439/1439 [==============================] - 535s 372ms/step - loss: 0.1138 - val_loss: 0.1109\n",
      "Epoch 4/20\n",
      "1439/1439 [==============================] - 537s 373ms/step - loss: 0.1095 - val_loss: 0.1069\n",
      "Epoch 5/20\n",
      "1439/1439 [==============================] - 536s 372ms/step - loss: 0.1055 - val_loss: 0.1024\n",
      "Epoch 6/20\n",
      "1439/1439 [==============================] - 528s 367ms/step - loss: 0.1007 - val_loss: 0.0990\n",
      "Epoch 7/20\n",
      "1439/1439 [==============================] - 533s 370ms/step - loss: 0.0986 - val_loss: 0.0983\n",
      "Epoch 8/20\n",
      "1439/1439 [==============================] - 535s 372ms/step - loss: 0.0972 - val_loss: 0.0970\n",
      "Epoch 9/20\n",
      " 666/1439 [============>.................] - ETA: 4:11 - loss: 0.0962"
     ]
    }
   ],
   "source": [
    "autoencoder.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "\n",
    "# train\n",
    "num_train_steps = len(Xtrain) // BATCH_SIZE\n",
    "num_test_steps = len(Xtest) // BATCH_SIZE\n",
    "#checkpoint = ModelCheckpoint(\n",
    "#   filepath=os.path.join(DATA_DIR, \"sent-thoughts-autoencoder.h5\"),\n",
    "#    save_best_only=True)\n",
    "history = autoencoder.fit_generator(train_gen,\n",
    "                                    steps_per_epoch=num_train_steps,\n",
    "                                    epochs=NUM_EPOCHS,\n",
    "                                    validation_data=test_gen,\n",
    "                                    validation_steps=num_test_steps) \n",
    "                                    #callbacks=[checkpoint, tensorboard])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect autoencoder predictions for test set\n",
    "test_inputs, test_labels = next(test_gen)\n",
    "preds = autoencoder.predict(test_inputs)\n",
    "\n",
    "# extract encoder part from autoencoder\n",
    "encoder = Model(autoencoder.input,\n",
    "                autoencoder.get_layer(\"encoder_lstm\").output)\n",
    "# encoder.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"loss\"], label = \"training loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label = \"validation loss\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute difference between vector produced by original and autoencoded\n",
    "k = 500\n",
    "cosims = np.zeros((k))\n",
    "i = 0\n",
    "for bid in range(num_test_steps):\n",
    "    xtest, ytest = next(test_gen)\n",
    "    ytest_ = autoencoder.predict(xtest)\n",
    "    Xvec = encoder.predict(xtest)\n",
    "    Yvec = encoder.predict(ytest_)\n",
    "    for rid in range(Xvec.shape[0]):\n",
    "        if i >= k:\n",
    "            break\n",
    "        cosims[i] = compute_cosine_similarity(Xvec[rid], Yvec[rid])\n",
    "        if i <= 10:\n",
    "            print(cosims[i])\n",
    "        i += 1\n",
    "    if i >= k:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(cosims, bins=10, density=True)\n",
    "plt.xlabel(\"cosine similarity\")\n",
    "plt.ylabel(\"frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
